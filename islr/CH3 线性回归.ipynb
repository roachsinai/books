{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 简单线性回归\n",
    "\n",
    "模型假设/Gauss-Markov假设：\n",
    "\n",
    "1. $E(\\epsilon )=0$  这条假设保证截距项a是可识别的。\n",
    "2. $var(\\epsilon)=\\sigma ^{2}$ 方差齐性假设，保证最小二乘估计是最优的，(Gauss-Markov定理)。即在所有无偏估计中方差最小。\n",
    "3. $\\epsilon$ 与$x$独立 这条假设保证最小二乘估计是无偏的。\n",
    "\n",
    "[最小二乘假设有哪些？各自有什么作用](https://www.zhihu.com/question/21264353/answer/17716432)\n",
    "\n",
    "## 3.1.1 估计系数\n",
    "\n",
    "残差平方和（residual sum of squares, RSS） $RSS=\\sum_{i=1}^n(y_i-\\hat{y_i})^2$\n",
    "\n",
    "根据假设得到的$\\hat{\\beta}_0$,$\\hat{\\beta}_1$是$\\beta_0$,$\\beta_1$无偏估计，而$\\hat{\\sigma}^2=\\frac{RSS}{n-2}$也是$\\sigma^2$的偏估计。\n",
    "\n",
    "$\\hat{\\beta}_0$,$\\hat{\\beta}_1$服从正太分布，可以进行**假设检验**。并且$\\frac{RSS}{n-2}\\~\\chi^2_{(n-2)}$，样本大小为$n$，因为有两个未知参数$\\beta_0$,$\\beta_1$需要估计，用掉了$2$个自由度。\n",
    "\n",
    "并且可以通过对$\\hat{\\beta}_1$的检验来验证$X$和$Y$之间是否有一定关系，体现在$\\hat{\\beta}_1$的$p$值上。\n",
    "\n",
    "## 3.1.3 评价模型的准确性\n",
    "\n",
    "+ 残差标准误（RSE）是对$\\epsilon$的标准方差的估计，所以$RSE=\\hat{\\sigma}$，表示观测值和真正回归直线（不是拟合的直线）的平均偏离，可以通过响应变量的均值的比值（误差百分比）作为一个评价\n",
    "+ 总平方和（total sum of squares, TSS）$TSS=\\sum (y_i-\\bar{y})$\n",
    "\n",
    "RSE被认为是模型**失拟**（lack of fit）的度量。如果对$i=1,...,n,$有$\\hat{y}_i\\approx y_i$，那么RSE的值会很小，可以认为魔性很好的拟合了数据。另外，如果在一个或多个预测中，$\\hat{y}_i 与 y_i$相差很大，那么RSE可能相当大，表明该模型没有很好的拟合数据。\n",
    "\n",
    "---\n",
    "\n",
    "$R^2$统计量\n",
    "\n",
    "$$R^2=\\frac{TSS-RSS}{TSS}=1-\\frac{RSS}{TSS}$$\n",
    "\n",
    "+ TSS测量响应变量$Y$的总方差，可以认为是在执行回归分析之前响应变量中的固有变异性\n",
    "+ RSS测量的是进行回归后扔无法解释的变异性\n",
    "+ TSS-RSS测量的是相应变量进行回归之后被解释的（或被消除）的变异性\n",
    "+ $R^2$测量的是$Y$的变异中能被$X$解释的部分所占的比例\n",
    "\n",
    "$R^2$统计量接近1说明回归可以解释响应变量的大部分变异。接近0说明回归没有解释太多响应变量的变异，**这可能因为模型是错误的，也可能因为固有误差项$\\sigma^2$较大，抑或两者兼有**。\n",
    "\n",
    "$R^2$与RSE相比**更易于解释**，因为它总是在0和1之间，而RSE则不然。但是，对$R^2$确定一个阈值来评价模型的好坏还是一个有难度的工作。例如在某些物理学问题中，我们可能知道数据实际上来自一个残差很小的线性模型，在这种情况下，我们预测$R^2$值将非常接近1，$R^2$值较小则可能表明生成数据的实验存在严重问题。另一方面，在生物学、心理学、市场营销和其他领域的典型应用中，线性模型最多是对数据的一个极其粗糙的近似，**受其他不可测因素影响**，残差往往也很大。在这种情况下，我们预期只有很小一部分响应变量的方差可以被预测变量所解释，所以远低于0.1的$R^2$值**可能更现实**。\n",
    "\n",
    "在简单线性模型中$R^2=r^2$，$r$为$X$和$Y$的相关系数。但是对于多元线性回归问题则不适用（这一概念**没有**自动扩展到多元情景中），因为相关性衡量的是一对变量之间的关系。\n",
    "\n",
    "$R^2$在多元回归中承担起了评价线性回归的模型拟合度的任务。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 多元线性回归\n",
    "\n",
    "## 3.2.2 一些重要的问题\n",
    "\n",
    "### 1. 相应变量和预测变量之间是否有关系\n",
    "\n",
    "+ 在简单的线性回归中，为确定响应变量和预测变量是否有关，可以简单的检验$\\beta_1$是否为0.\n",
    "+ 在有$p$个预测变量的多元回归模型中，问的则是所有的回归系数是否均为零，即$\\beta_1=\\beta_2=...= \\beta_p=0$是否成立。\n",
    "\n",
    "零假设$H_0$成立，误差项$\\epsilon_i$服从正态分布时，**F统计量**\n",
    "\n",
    "$$F=\\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$$\n",
    "\n",
    "服从$\\mathbb{F}$分布。\n",
    "\n",
    "因为，如果线性回归的假设是正确的，可知$E\\{RSS/(n-p-1)\\}=\\sigma^2$。**进一步**如果$H_0$为真，则$E\\{TSS-RSS/p\\}=\\sigma^2$。所以，如果响应变量和预测变量无关，$F$统计量应该接近1。如果$H_a$（备择假设）为真，那么$E\\{TSS-RSS/p\\}>\\sigma^2$（因为有**比较明显的误差被回归消除**），所以$F$统计量预计大于$1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "但是，有的时候我们想要检验的是：系数的规模为$q$的特定子集为零。方便起见，将选择忽略（假设系数为0）的变量放在列表的末尾，零假设为：\n",
    "\n",
    "$$\\beta_{p-q+1}=\\beta_{p-q+2}=...= \\beta_p=0$$\n",
    "\n",
    "这种情况下需要用去掉最后$q$个变量之外的所有变量建立第二个模型。假设模型的残差平方和为$RSS_0$，则相应的$F$统计量为：\n",
    "\n",
    "$$F=\\frac{(RSS_0-RSS)/q}{RSS/(n-p-1)}$$\n",
    "\n",
    "---\n",
    "\n",
    "但是，有一点需要注意，在多元线性回归中，输出模型系数时，也会得到每个系数的**值**、**标准误**、**$t$统计量**、**$p$值**。这些统计量提供的信息表明了**在控制其他所有变量之后，每个预测变量与响应变量是否相关**。\n",
    "\n",
    "事实证明，每个变量的$t$检验都等价于**不含该变量，但包含所有其他变量的模型的$F$检验**（每个$t$统计量的平方，就是相应的$F$统计量）。因此，它反映了该变量加入模型的偏（其他变量不变）效应。\n",
    "\n",
    "那么，既然多元线性回归可以得到每个变量的$p$值，那么如果有一个变量的$p$值显著，**是不是就不用**看$F$统计量了呢？\n",
    "\n",
    "不是，这种逻辑有缺陷，特别是当**预测变量的数目很大**的时候。\n",
    "\n",
    "考虑一个例子，假设$p=100$，且零假设$\\beta_1=\\beta_2=...= \\beta_p=0$为真，这事实上说明没有任何预测变量与响应变量相关。但是，每个变量$t$检验的$p$值中，有约$5%$将碰巧小于$0.05$。事实上，我们几乎肯定能观察到至少一个碰巧小于$0.05$的$p$值。但是$F$统计量就没有这个问题，它会根据预测变量的个数进行调整。若$H_0$为真，无论预测变量的个数或观测个数是多少，$F$统计量的$p$值小于$0.05$的概率只有$5%$。\n",
    "\n",
    "---\n",
    "\n",
    "若$p>n$，则待估系数个数多于用于估计的观测个数，这时**甚至不能用最小二乘法拟合多元线性模型**，所以$F$统计量无法使用，本章中的其他大多数概念也不行！这时需要特征选择。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 选定重要的变量\n",
    "\n",
    "判断模型质量的准则：\n",
    "\n",
    "+ Mallow's统计量$C_p$\n",
    "+ 赤池信息准则（Akaike information criterion, AIC）\n",
    "+ 贝叶斯信息准则（Bayesian information criterion, BIC）\n",
    "+ 调整$R^2$（adjusted $R^2$）\n",
    "\n",
    "变量选择的经典方法：\n",
    "\n",
    "+ 向前选择（forward selection）。从零模型（null model）——只含有截距但不含预测变量的模型开始。然后建立简单线性回归模型，把使得$RSS$最小的变量添加到零模型中。之后再加入一个变量，得到新的双变量模型。直到这个过程满足某种停止规则为止。\n",
    "+ 向后选择（backward selection）。先从包含所有变量的模型开始，并删除$p$值最大的变量——统计学上最不显著的变量。拟合完包含$(n-1)$个变量的新模型之后再删除$p$值最大的变量。此过程直到满足某种停止规则为止。比如，所有剩余变量的$p$值低于摸个阈值。\n",
    "+ 混合选择（mixed selection）。这是向前选择和向后选择的综合。和向前选择相同，从不含变量的模型开始，想模型中添加你和最好的变量，然后依次增加变量。正如在$Advertising$例子中看到的，想模型中加入新的变量后，**有的变量的$p$值可能会变大**。因此，一旦模型中，某个变量的$p$值超过一定的阈值时，就从该模型中删除该变量。不断执行这些向前或向后的步骤，直到模型中所有变量的$p$值均低于某个阈值，且**模型外的任何变量加入模型后都将有较大的$p$值**。\n",
    "\n",
    "$tips$\n",
    "\n",
    "+ 当$p>n$时，不能使用后向选择\n",
    "+ 向前选择在各种情况下都可用\n",
    "+ 向前选择是一种贪婪的方法，它可能在前期将后来变得多雨的变量纳入模型。**混合选择**可以修正这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. 模型拟合\n",
    "\n",
    "两个最常见的衡量模型拟合优劣的指标是$RSE$和$R^2$（方差的解释比例）。\n",
    "\n",
    "+ 在简单回归中，$R^2$是响应变量和预测变量的相关系数的平方。\n",
    "+ 在多元线性回归中，$R^2$等于$Cor(Y,\\hat{Y}^2)$，是响应变量和线性模型拟合值的相关系数的平方。\n",
    "\n",
    "事实上，线性拟合模型的一个特性就是，是所有的线性模型中使上述相关系数最大的模型。\n",
    "\n",
    "$R^2$总在$0$和$1$之间，更易于解释。但是，在进行模型拟合时，如果添加了新的变量，$R^2$总会增加（即使加入的变量和响应变量的关联很弱）。这是因为在最小二乘中添加变量必然会使我们能更准确地拟合训练数据（尽管对测试数据未必如此）。以Advertising数据为例子，三个广告媒体也测sales的模型的$R^2$为$0.897$，去掉$newspaper$的$R^2$则为$0,89719$。\n",
    "\n",
    "事实上，$newspaper$的加入只使得模型$newspaper$有极小的增加，充分表明$newspaper$变量可以被删除。从本质上讲，**$newspaper$变量没有真正地改善模型对训练样本的拟合，将其纳入模型可能导致模型出现过拟合，从而在独立测试样本上效果不佳**。\n",
    "\n",
    "是否加入一个变量到模型中，也可以通过对比加入前后模型的$RSE$的变化作出决定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$RSE$一般被定义为\n",
    "\n",
    "$$RSE=\\sqrt{\\frac{1}{n-p-1}RSS}$$\n",
    "\n",
    "在简单线性模型中$p=1$，所以，相对于$p$的增加来说，$RSS$的减少量较小，则变量较多的模型可能有更高的$RSE$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了**$RSE$**和**$R^2$**，进行**数据绘图**也是很有用的。图形摘要可以揭示模型中不能被数字统计所反应的问题。如果回归问题有两个预测变量，结合响应变量可以画出一个三维图。假如发现下面这种情况：预测变量加和不变，当某一个预测变量占大头，而另一个预测变量的值较小时，响应变量低于回归值；当两个预测变量相差不大时，响应变量高于回归平面（线性模型低估了响应变量）。这表明两个预测变量之间存在**协同作用**或**交互作用**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. 预测\n",
    "\n",
    "（1）**最小二乘平面**只是对**真实总体回归平面**的一个估计。系数估计的不准确性（最小二乘平面）与**可约误差**有关。可以通过计算**置信区间**来确定回归值$\\hat{y}$和$f(X)$（真实回归平面的取值）的接近程度。\n",
    "\n",
    "最小二乘平面：\n",
    "\n",
    "$$\\hat{Y}=\\hat{\\beta}_0+\\hat{\\beta}_1X_1+\\hat{\\beta}_2X_2+...+\\hat{\\beta}_pX_p$$ \n",
    "\n",
    "真实总体回归平面:\n",
    "\n",
    "$$f(X)=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_pX_p$$ \n",
    "\n",
    "（2）假设的线性模型$f(X)$几乎总是对现实的一种近似，存在改进可约误差的机会。**线性模型假设本身**就是可约误差的一种来源，称之为**模型偏差**。所以在使用线性模型时，我们其实是在对真是平面进行**最佳线性近似**。\n",
    "\n",
    "（3）即使$f(X)$已知，就是说真实总体回归平面的所有系数真实值已知，我们同样不能对真实总体回归平面的$Y$值做出完美的预测，因为模型中存在随机误差$\\epsilon$（不可约误差）。至于$\\hat{Y}$和$Y$相差多少，使用**预测区间**（prediction interval）来回答这个问题。预测区间总是比置信区间宽，因为预测区间既包含$f(X)$的估计误差（可约误差），又包含单个点偏离总体回归平面的不确定性。\n",
    "\n",
    "比如，在$Advertising$数据中，通过$TV,Radio$建立好模型之后，使用置信区间对大量城市的平均$sales$的不确定性进行量化。使用预测区间对某个特定城市的$sales$的不确定性进行特定量化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.3 回归模型中的其他注意事项\n",
    "\n",
    "### 3.3.1 定性预测变量\n",
    "\n",
    "对于$qualitative predictor$使用哑变量\n",
    "\n",
    "### 3.3.2 线性模型的扩展\n",
    "\n",
    "线性模型提供了可解释的结果，但是也做了一些高度限制的假设。两个最重要的假设是预测变量和响应变量关系是可加（additive）和线性（linear）的。\n",
    "\n",
    "+ **可加**性指预测变量$X_j$的变化对响应变量$Y$产生的影响与其他预测变量的取值无关。\n",
    "+ 线性假设是指无论$X_j$取何值（高或者低），$X_j$变化一个单位引起的响应变量$Y$的变化是恒定的。\n",
    "\n",
    "下面就从两个方面扩展线性模型。\n",
    "\n",
    "**去除可加性假设**\n",
    "\n",
    "正如$3.2.2$的第$3$部分最后所述，有的预测变量之间存在在营销中称为协同（synergy）效应的现象，它在统计学中称为交互作用（interaction）。比如假设广播广告的投入事实上增强了电视广告的有效性，这时$TV$的系数应随着$radio$的增加而增加。\n",
    "\n",
    "对于有两个预测变量的情况，一种扩展线性模型，使其能考虑交互作用的方法是加入第三个预测变量，名为**交互项**（interaction term），交互项由$X_1$和$X_2$的乘积构成。可以得到模型：\n",
    "\n",
    "$$Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_1X_2+\\epsilon$$ \n",
    "\n",
    "该模型是如何放松可加性假设的呢？\n",
    "\n",
    "$$Y=\\beta_0+(\\beta_1+\\beta_3X_2)X_1+\\beta_2X_2+\\epsilon$$ \n",
    "\n",
    "$$Y=\\beta_0+\\tilde{\\beta}_1X_1+\\beta_2X_2+\\epsilon$$ \n",
    "\n",
    "因为$\\tilde{\\beta}_1$随$X_2$变化，所以$X_1$对$Y$的效应（effect）不再是常数：调整$X_2$的值将影响$X_1$对$Y$的影响。$t$检验也可以证明交互项是否显著，$R^2$则可以表明有交互项的模型是否由于仅包含主效应的模型。\n",
    "\n",
    "**实验分层原则**（hierarchical principle）：如果模型中含有交互项，那么即使主效应的系数的$p$值不显著，也应包含在模型中。原则的合理性在于，$X_1xX_2$通常$X_1$和$X_2$相关，但是又表示出了不可加的部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**非线性关系**\n",
    "\n",
    "这里简要的提一下使用多项式回归。但是非线性增加，如果在图示中发现拟合的曲线有不必要的波动，那么就不清楚引入的非线性是否真的带来了更好的数据拟合。\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3.3 潜在的问题\n",
    "\n",
    "+ 非线性的响应-预测关系\n",
    "+ 误差项自相关\n",
    "+ 误差项方差非恒定\n",
    "+ 离群点\n",
    "+ 高杠杆点（high-leverage point）\n",
    "+ 共线性（collinearity）\n",
    "\n",
    "**在实践中，识别和解决这些问题是一门科学，也是一门艺术**。\n",
    "\n",
    "1. 数据的非线性\n",
    "\n",
    "    如果响应-预测的真实关系是非线性的，那么线性模型得出的所有结论几乎都是不可信的，模型的预测精度也可能显著降低。\n",
    "    \n",
    "    残差图（residual plot）是一种很有用的图形工具，可以用于识别非线性。在简单线性回归模型中，我们可以绘制残差$e_i=y_i-\\hat{y}_i$与预测变量$x_i$的散点图。在多元回归中，因为有多个预测变量，我们绘制残差与预测值（或拟合值（fitted））$\\hat{y}_i$的散点图（横坐标是拟合值，而不是样本$x$）。理想情况下，残差图显示不出明显规律。若存在明显规律，则表示线性模型的有些方面可能存在问题。\n",
    "    \n",
    "    如果残差呈现明显的$U$形，这位数据的非线性提供了强有力的证据，一个简单的方法是对模型中使用的预测变量进行非线性变换，如$logX,\\sqrt{X},X^2$。\n",
    "    \n",
    "2. 误差自相关\n",
    "    \n",
    "    线性模型的一个重要假设是误差项$\\epsilon_1,\\epsilon_2,...,\\epsilon_n$不相关。误差项不想管，那么$\\epsilon_i$为正这一事实完全不能（或几乎不能）为判断$\\epsilon_{i+1}$的符号提供任何信息。而回归系数和拟合值的标准误的计算都基于误差项不相关的假设。\n",
    "    \n",
    "    如果误差项之间有相关性，那么估计标准误往往低估了真实标准误。因此，置信区间和预测区间比真实区间要窄。而$95%$置信区间包含真实参数的实际概率将远低于$0.95$。模型$p$也比真实的更低，这个能导致我们得出错误的结论，认为参数是统计显著的。\n",
    "    \n",
    "    举一个例子，如果不小心将数据重复了一遍，那么学习模型时，似乎是在计算一个规模为$2n$的样本的标准误，但事实上样本量仅为$n$。$2n$和$n$个样本的参数估计是相同的，但是后者的置信区间的宽度是前者的$\\sqrt{n}$倍!\n",
    "    \n",
    "    比如在下面左侧的图片中，前四个点的误差都是正值，后面两个点是负误差。这样在拟合时，拟合的平面理应在前四个样本之下，后两个样本之上。但是最小二乘没有这样的假设，就出现了一个拟合的效果“更好”的平面，这样误差的方差最小。但是，事实上并不是，右侧也是这样。[Serial Correlation](https://www3.nd.edu/~rwilliam/stats2/l26.pdf)\n",
    "    \n",
    "    ![](img/serial_correlation.png)\n",
    "    \n",
    "    通常，误差项会出现在时间序列数据中，即在离散时间点测量得到的观测构成的数据中。在很多情况下，在相邻的时间点获得的观测的误差有正相关关系。为了确定某一给定的数据集是否有这种问题，可以根据拟合的模型绘制作为时间函数的残差。如果误差不相关，那么图中不会有明显的规律。\n",
    "    \n",
    "    但是，自相关性也会出现在时间序列以外的数据中。比如一个使用体重预测身高的研究中，如果研究中有些人是同一个家庭的成员（大高个就是瘦），或者饮食习惯相同，或者暴露于同样的环境因素下，误差不相关的假设就可能被违反。\n",
    "    \n",
    "    通常，误差不相关对线性回归和其他的统计方法来说是非常重要的。\n",
    "    \n",
    "    ---\n",
    "    \n",
    "3. 误差项方差非恒定\n",
    "    \n",
    "    线性模型中的假设检验和标准误差、置信区间的计算都依赖于这一假设。Unfortunately，通常情况下，误差项的方差不是恒定的。\n",
    "    \n",
    "    比如，误差项的方差，可能随响应值的增加而增加。如果残差图**呈漏斗状**（横坐标一头的残差非常靠近$residual=0$这条水平线，相反另一侧远离，比如残差随拟合值的增加而增加），说明误差方差非恒定或存在异方差性。残差随拟合值的增加而增加的一个可能的解决方法是用凹函数对响应值$y$做变换，比如$logY$或$\\sqrt{Y}$。这种变换使得较大的响应值有更大的收缩，降低了异方差性。\n",
    "    \n",
    "    有时我们可以估计每个响应值的方差。例如，第$i$个响应值可能是$n_i$个原始观测值的平均值。如果每个原始观测都与方差$\\sigma^2$无关，那么它们均值的方差是$\\sigma_i^2=\\sigma^2/n_i$。这时，一个简单的补救方法是用加权最小二乘法拟合模型，即设置样本的权重与方差的倒数成比例——本例中即$w_i=n_i$，使用权重改变方差不一致的情况。\n",
    "\n",
    "    ---\n",
    "\n",
    "4. 离群点\n",
    "    \n",
    "    离群点是指$y_i$远离模型预测值的点。产生离群点的原因很多，比如错误的记录。\n",
    "    \n",
    "    有时候离群点对最小二乘的结果（系数）只有很微小的影响。但是，即使这样它会导致其他问题。比如会提高$RSE$，提高$1/5$，由于$RSE$是计算所有置信区间和$p$值的依据，单个数据点造成它们的急剧增加可能影响对拟合的解释。同样，加入离群点会导致$R^2$的下降。\n",
    "    \n",
    "    有时候残差图可以直接显示出离群点。但是，通常实践中，确定残差多大的点是一个离群点会很困难。而**学生化残差**（studentized residual）——由残差$e_i$除以估计标准误得到，其绝对值大于$3$的观测点可能是离群点。\n",
    "    \n",
    "    如果确信某个离群点是由数据采集或记录中的错误导致的，那么一个解决方案是直接删除此观测点。但是值得小心，因为一个离群点可能不是失误导致的，而是暗示模型存在缺陷，比如缺少预测变量。\n",
    "    \n",
    "    ---\n",
    "\n",
    "5. 高杠杆点\n",
    "\n",
    "    离群点是对于给定的$x_i$来说，响应值$y_i$异常的点。而**高杠杆点**表示观测点$x_i$是异常的。\n",
    "    \n",
    "    之前说了离群点可能对线性拟合的效果（但是会导致其他的问题），高杠杆的观测往往对回归直线的估计有很大的影响（可以想象，你在做拟合而这个拟合直线的一个远端点已经确定了（两点确定一条直线），也就是你一堆点只确定了拟合直线的一个点）。\n",
    "    \n",
    "    在简单线性回归中，高杠杆值是很容易辨认的（只有一个变量）。但是，在多元回归中，可能有这样的观测点：单独来看，它各个预测变量的取值都在正常范围内，但是从整个预测变量的角度来看，它却是不寻常的。比如多数样本是分布在对角线上，那么分布在右下角的就是高杠杆值。\n",
    "    \n",
    "    为了量化观测的杠杆作用，可以计算**杠杆统计量**（leverage statistic）。一个大的杠杆统计量对应一个高杠杆点。对于简单线性回归有\n",
    "    \n",
    "    $$h_i=\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{\\sum_{i'=1}^n(x_{i'}-\\bar{x})^2}$$\n",
    "    \n",
    "    从方程中可以看出$h_i$随着$x_i-\\bar{x}$的增加而增加。杠杆统计量$h_i$的取值总是在$1/n$和$1$之间，且所有观测的平均杠杆值总是等于$(p+1)/n$。因此，如果给定观测的杠杆统计量大大超过$(p+1)/n$，那就可能怀疑对应点有较高的杠杆作用。\n",
    "    \n",
    "    多的杠杆值（样本分布的比较散），使得$SE(\\hat{\\beta}_1)$更小（从公式可以看出，多杠杆值利于估计线性模型的斜率）；但是高杠杆值使得模型偏离真实回归面还要严重与outlier。\n",
    "    \n",
    "    ----\n",
    "\n",
    "6. 共线性（collinearity）\n",
    "\n",
    "    共线性是指两个或更多的预测变量高度相关。共线性在散点图中会有体现。\n",
    "    \n",
    "    数据中如果存在（多重）共线性，那么资料矩阵是不可逆的，那么解就不唯一，会有多个解。如果是近似共线性，那么$RSS$的等高线就类似一个狭长的山谷，每个等高线是取得相同$RSS$的不同估计系数，等高线中心是最小二乘估计。而有共线性时，狭长的山谷使得有共线性的任一预测变量样本的微小变化可能导致$RSS$中心沿着任何方向移动。这导致估计的系数存在很大的不确定性（毕竟，接近有很多个解）。\n",
    "    \n",
    "    由于共线性降低了回归系数的准确性，它会导致$\\hat{\\beta}_j$的标准误变大，会导致$t$统计量的下降。所以如果数据存在共线性，我们可能无法拒绝$H_0:\\beta_j=0$。这意味着，检验的效力（power）——正确的检测出非零系数的概率——被共线性减小了。\n",
    "    \n",
    "    但是，感觉存在共线性导致的这些问题是正常的——**毕竟有一部分信息被多个变量分摊了，就不显著了**。但是，由于共线性存在的问题，就需要去除数据的共线性。\n",
    "    \n",
    "    共线性可以简单的使用相关系数矩阵检测。但是，并非所有的共线性问题都可以通过检查相关系数矩阵检测到：即使没有某对变量具有特别高的相关性，有可能三个或更多变量之间存在共线性——多重共线性（multicoollinearity）。\n",
    "    \n",
    "    一个更好的评估多重共线性的方式是计算**方差膨胀因子**（variance inflation factor，VIF）。$VIF$是拟合全模型时的系数$\\hat{\\beta}_j$的方差除以单变量回归中$\\hat{\\beta}_j$的方差所得的比例。$VIF$的最小可能值是$1$，表示完全不存在共线性。通常，$VIF$超过$5$或$10$表示有共线性问题。$VIF$可以使用下面的公式进行计算：\n",
    "    \n",
    "    $$VIF(\\hat{\\beta}_j)=\\frac{1}{1-R_{X_j|X_{-j}}^2}$$\n",
    "    \n",
    "    其中$R_{X_j|X_{-j}}^2$是$X_j$为响应变量，对其他所有预测变量的回归的$R^2$。如果$R_{X_j|X_{-j}}^2$接近与$1$，表明$X_j$可以很好的被其他变量拟合，那么存在共线性，而且$VIP$会很大。\n",
    "    \n",
    "    解决共线性的方法，通常有两种：\n",
    "    \n",
    "    + 因为共线性说明在其他变量存在的前提下，此变量提供有关响应的信息是多余的。剔除即可。\n",
    "    + 将共线变量组合成一个单一的预测变量。比如，标准化之后求平均得到一个新的变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3.4 营销计划\n",
    "\n",
    "1. 广告预算和销量有关么？\n",
    "\n",
    "    即$F$检验是否所有预测变量的系数全为$0$。\n",
    "    \n",
    "2. 广告预算和销售间的关系有多强？\n",
    "    + $RSE$估计了响应偏离总体回归直线的标准差\n",
    "    + $R^2$ 统计量记录了预测变量解释的响应变量的百分比\n",
    "\n",
    "3. 哪种媒体能促进销售？\n",
    "\n",
    "    看回归模型每个预测变量的$t$统计量。\n",
    "\n",
    "4. 如何精确地估计每种媒体对销售的影响？\n",
    "\n",
    "    看回归模型每个预测变量系数的置信区间（是否包含$0$，置信区间是不是非常宽（相对其他变量））。也可以对每个预测变量做简单线性回归（考虑，可能存在共线性）。\n",
    "\n",
    "5. 对未来销量的预测精度如何？\n",
    "\n",
    "    响应变量可以使用$\\hat{Y}=\\hat{\\beta}_0+\\hat{\\beta}_1X_1+\\hat{\\beta}_2X_2+...+\\hat{\\beta}_pX_p$来预测。估计的准确性取决于我们想预测的是单个响应值$Y=f(X)+\\epsilon$，还是平均响应$f(X)$。如果是前者，使用**预测区间**，如果是后者，使用**置信区间**。预测区间解释了$\\epsilon$的不确定性。\n",
    "\n",
    "6. 这种关系是否是线性的？\n",
    "\n",
    "    可以使用残差图识别非线性。可以通过变换成为线性的形式。\n",
    "\n",
    "7. 广告媒体间是否存在协同效应？\n",
    "\n",
    "    看交互项的$p$值。加入交互项之后$R^2$的增加与否。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3.5 线性回归与$K$近邻法的比较\n",
    "\n",
    "这两个的对比也可以看作是参数检验和非参数检验的对比。\n",
    "\n",
    "参数检验容易拟合，但是对$f(X)$的形式提出了很强的假设，非参数检验不明确假设一个参数化的形式$f(X)$，从而提供了一种更灵活的代替方法来进行回归（比如$K$近邻回归，$K$值越大拟合越光滑）。\n",
    "\n",
    "而$K$值的选择则又是偏差-方差的均衡。\n",
    "\n",
    "那么什么情况下，参数方法会优于非参数方法？如果选定的参数形式接近$f$的真实形式，则参数方法更优。\n",
    "\n",
    "在本书的示例中，当真实关系为线性时，$KNN$略逊于线性回归，但是非线性情况下，$KNN$大大优于线性回归。但是，**现实中，即使真实关系是高度非线性的，$KNN$的结果仍有可能比线性回归更差**。在低维还可以接受，更高维的情况$KNN$表现往往不如线性回归。\n",
    "\n",
    "维度的增加会使得数据中噪声对$KNN$的影响显著增加。预测效果随着维数的增加而恶化是$KNN$的一个普遍问题，这是因为**高维中样本量大大减少**（相较维度）。$100$个训练样本分布在$20$个维度上时，将使得给定的观测点附近没有邻点（nearby neighbors）——维数灾难（curse of dimensionality）。所以，**高维数据需要大量的样本**（体现出$SVM$的价值）。\n",
    "\n",
    "+ 如果每个预测变量仅有少量观测，参数化方法往往优于非参数化方法。\n",
    "+ 预测精度参不多时，可能放弃一些预测精度，选择能被几个系数描述且系数的$p$值都可知的简单模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
